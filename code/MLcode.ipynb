{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e51c2d6",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-setup-functions\" data-toc-modified-id=\"Imports-and-setup-functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and setup functions</a></span></li><li><span><a href=\"#DataLoader-Module\" data-toc-modified-id=\"DataLoader-Module-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>DataLoader Module</a></span><ul class=\"toc-item\"><li><span><a href=\"#Image-regions\" data-toc-modified-id=\"Image-regions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Image regions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-interesting-regions-to-test-on\" data-toc-modified-id=\"Find-interesting-regions-to-test-on-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Find interesting regions to test on</a></span></li><li><span><a href=\"#Cut-all-data-into-64x64-regions\" data-toc-modified-id=\"Cut-all-data-into-64x64-regions-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Cut all data into 64x64 regions</a></span></li><li><span><a href=\"#Save-to-.npy-files\" data-toc-modified-id=\"Save-to-.npy-files-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Save to .npy files</a></span></li></ul></li><li><span><a href=\"#Train-val-test-split\" data-toc-modified-id=\"Train-val-test-split-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Train-val-test split</a></span></li><li><span><a href=\"#Inputs-and-ends\" data-toc-modified-id=\"Inputs-and-ends-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Inputs and ends</a></span></li><li><span><a href=\"#Tensors,-DataLoader\" data-toc-modified-id=\"Tensors,-DataLoader-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Tensors, DataLoader</a></span></li><li><span><a href=\"#Create-DataLoader-for-training-data\" data-toc-modified-id=\"Create-DataLoader-for-training-data-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Create DataLoader for training data</a></span></li></ul></li><li><span><a href=\"#Model-Module\" data-toc-modified-id=\"Model-Module-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model Module</a></span><ul class=\"toc-item\"><li><span><a href=\"#AR1:-linear-mapping\" data-toc-modified-id=\"AR1:-linear-mapping-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>AR1: linear mapping</a></span></li><li><span><a href=\"#de-Bézenac-et-al,-2019:-CNN-with-warp-mapping\" data-toc-modified-id=\"de-Bézenac-et-al,-2019:-CNN-with-warp-mapping-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>de Bézenac et al, 2019: CNN with warp mapping</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loss-Function\" data-toc-modified-id=\"Loss-Function-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Loss Function</a></span></li><li><span><a href=\"#Warp\" data-toc-modified-id=\"Warp-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Warp</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Training</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb70983",
   "metadata": {},
   "source": [
    "# Imports and setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6754139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Function, Variable\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2088495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datadir(x):\n",
    "    return \"/projectnb/labci/Lucia/data/\" + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f15e4f",
   "metadata": {},
   "source": [
    "# DataLoader Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8f21d",
   "metadata": {},
   "source": [
    "## Train-val-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c5887",
   "metadata": {},
   "source": [
    "The authors trained on SST data from 2006-2015 and tested on data from 2016-2017. For the IBI reanalysis SST data, we only have from June 5, 2021 to June 23, 2023 (749 days). From these, we will use 80% for training, 10% for validation, and 10% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d16162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_days(files):\n",
    "    \"\"\"Helper method to take a list of filenames and return total the number \n",
    "    of days represented by the files in the list. \n",
    "    \n",
    "    This method assumes a particular file naming convention that has been used \n",
    "    for this project. \n",
    "    \n",
    "    File naming convention: sst_geo_yyyymmdd.nc_region_XX.npy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list : data file names\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    delta : int : the number of days represented by the inputted files\n",
    "    \"\"\"\n",
    "    \n",
    "    files.sort()\n",
    "    \n",
    "    first = datetime.datetime.strptime(files[0][8:16], \"%Y%m%d\").date()\n",
    "    last = datetime.datetime.strptime(files[len(files)-1][8:16], \"%Y%m%d\").date()\n",
    "    \n",
    "    delta = int((last - first) / datetime.timedelta(days=1))\n",
    "    \n",
    "    return delta\n",
    "\n",
    "def train_val_test_cutoffs(topdir, split):\n",
    "    \"\"\"Helper method to create lists of filenames for the train, val, and test\n",
    "    data splits. Uses the dates in the filenames to determine file order and \n",
    "    split cutoffs.\n",
    "    \n",
    "    This method assumes a particular file naming convention that has been used \n",
    "    for this project. \n",
    "    \n",
    "    File naming convention: sst_geo_yyyymmdd.nc_region_XX.npy\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    topdir : string : the path to the directory holding the data\n",
    "    split : list : the fractions for each split, eg. [0.8, 0.1, 0.1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cutoffs : list : date cutoffs for each split\n",
    "    \"\"\"\n",
    "    \n",
    "    all_files = os.listdir(topdir)\n",
    "    all_files.sort()\n",
    "    nfiles = len(all_files)\n",
    "    ndays = all_days(all_files)\n",
    "    \n",
    "    start_date = datetime.datetime.strptime(all_files[0][8:16], \"%Y%m%d\").date()\n",
    "    end_date = datetime.datetime.strptime(all_files[nfiles-1][8:16], \"%Y%m%d\").date()\n",
    "    \n",
    "    cutoffs = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        delta = math.floor(ndays*split[i])\n",
    "        end = start_date + datetime.timedelta(days=delta)\n",
    "        cutoffs.append(end)\n",
    "        \n",
    "        start_date = end\n",
    "        \n",
    "    # Because of rounding, some files may have been missed\n",
    "    # Add these to the test split\n",
    "    if cutoffs[2] < end_date:\n",
    "        cutoffs[2] = end_date\n",
    "    \n",
    "    return cutoffs\n",
    "\n",
    "def train_val_test_split_files(topdir, split):\n",
    "    \"\"\"Method to split the data in a directory into training, validation, and\n",
    "    test sets. Uses the helper method train_val_test_cutoffs().\n",
    "    \n",
    "    This method assumes a particular file naming convention that has been used \n",
    "    for this project. \n",
    "    \n",
    "    File naming convention: sst_geo_yyyymmdd.nc_region_XX.npy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    topdir : string : the path to the directory holding the data\n",
    "    split : list : the fractions for each split, eg. [0.8, 0.1, 0.1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train : list : list of filenames for the train set\n",
    "    val : list : list of filenames for the validation set\n",
    "    test : list : list of filenames for the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(split) == 3, \"Please include a % split for train, validation, and test sets.\"\n",
    "    \n",
    "    cutoffs = train_val_test_cutoffs(topdir, split)\n",
    "    \n",
    "    all_files = os.listdir(topdir)\n",
    "    all_files.sort()\n",
    "    \n",
    "    train, val, test = [], [], []\n",
    "    \n",
    "    for f in all_files:\n",
    "        file_date = datetime.datetime.strptime(f[8:16], \"%Y%m%d\").date()\n",
    "        \n",
    "        if file_date <= cutoffs[0]:\n",
    "            train.append(f)\n",
    "        elif (file_date > cutoffs[0]) & (file_date <= cutoffs[1]):\n",
    "            val.append(f)\n",
    "        else:\n",
    "            test.append(f)\n",
    "            \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bda7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(filename):\n",
    "    \"\"\"Helper method to load a single .npy file.\"\"\"\n",
    "    return np.load(datadir(\"sst_npy/\" + filename))\n",
    "\n",
    "\n",
    "def load_data_from_files(files):\n",
    "    \"\"\"Method to load data from a list of .npy files.\"\"\"\n",
    "    data = []\n",
    "    for f in files:\n",
    "        data.append(load_data_from_file(f))\n",
    "        \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2117c9",
   "metadata": {},
   "source": [
    "## Inputs and ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a6a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_region(files, region):\n",
    "    \"\"\"Helper method to search a list of files for only those corresponding to\n",
    "    a desired region. \n",
    "    \n",
    "    This method assumes a particular file naming convention that has been used \n",
    "    for this project. \n",
    "    \n",
    "    File naming convention: sst_geo_yyyymmdd.nc_region_XX.npy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list : list of filenames in which to search\n",
    "    region : int : desired region\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    region_files : list : sorted list of matching files\n",
    "    \"\"\"\n",
    "    \n",
    "    region_files = []\n",
    "    for fname in files:\n",
    "        if \"region_\" + str(region) + \".npy\" in fname:\n",
    "            region_files.append(fname)\n",
    "            \n",
    "    region_files.sort()\n",
    "    \n",
    "    return region_files\n",
    "\n",
    "def get_pairs_by_region(files, region, ndays=1):\n",
    "    \"\"\"Method to separate data into inputs (X) and ends (y), for example to\n",
    "    use 4 previous days (ndays=4) to predict the next day. The \"pairs\" are \n",
    "    pairs of (X,y) inputs and ends. This method works on one region at a time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list : list of files from which to get pairs\n",
    "    region : int : desired region\n",
    "    ndays : int : number of days to use as inputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    inps : np.ndarray : filenames for inputs\n",
    "    ends : list : filenames for ends\n",
    "    \"\"\"\n",
    "    \n",
    "    region_files = search_by_region(files, region)\n",
    "    \n",
    "    n = len(region_files)\n",
    "    \n",
    "    inps = []\n",
    "    ends = region_files[ndays:]\n",
    "    \n",
    "    for i in range(n - ndays):\n",
    "        inps.append(region_files[i:i+ndays])\n",
    "    \n",
    "    return np.array(inps), np.array(ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43858887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_regions(files):\n",
    "    \"\"\"Helper method to take a list of filenames and return a list of the \n",
    "    unique region numbers. \n",
    "    \n",
    "    This method assumes a particular file naming convention that has been used \n",
    "    for this project. \n",
    "    \n",
    "    File naming convention: sst_geo_yyyymmdd.nc_region_XX.npy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list : data file names\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    regions : list : list of unique region numbers in the directory\n",
    "    \"\"\"\n",
    "    \n",
    "    files.sort()\n",
    "    regions = []\n",
    "    \n",
    "    for f in files:\n",
    "        if \"region_\" in f:\n",
    "            \n",
    "            start_ind = f.find(\"region_\") + len(\"region_\")\n",
    "            end_ind = f.find(\".npy\")\n",
    "            \n",
    "            reg = f[start_ind:end_ind]\n",
    "            \n",
    "            if int(reg) not in regions:\n",
    "                regions.append(int(reg))\n",
    "            \n",
    "        else:\n",
    "            print(\"Files in this directory do not match the naming convention.\")\n",
    "    \n",
    "    regions.sort()\n",
    "    \n",
    "    return regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510e497",
   "metadata": {},
   "source": [
    "## Tensors, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa7d662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(batchsize, files, ndays, dtype=torch.FloatTensor, shuff=True, all_data=True, regs=None):\n",
    "    \"\"\"Method to create a PyTorch DataLoader object from a list of files and\n",
    "    additional parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dtype : torch.dtype : the data type for the DataLoader\n",
    "    batchsize : int : the desired batchsize for loading data\n",
    "    files : str : a list of files holding data to put into the DataLoader\n",
    "    ndays : int : the number of days to use as inputs to predict the next day\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loader : torch.utils.data.DataLoader : the DataLoader object\n",
    "    \"\"\"\n",
    "    \n",
    "    if all_data:\n",
    "        regions = all_regions(files)\n",
    "    else:\n",
    "        regions = regs\n",
    "\n",
    "    data = []\n",
    "    ends = []\n",
    "\n",
    "    for reg in regions:\n",
    "        reg_pairs = get_pairs_by_region(files, reg, ndays)\n",
    "        \n",
    "        for i in range(len(reg_pairs[0])):\n",
    "            dat = load_data_from_files(reg_pairs[0][i])\n",
    "            end = load_data_from_file(reg_pairs[1][i])\n",
    "        \n",
    "            data.append(dat)\n",
    "            ends.append(end)\n",
    "        \n",
    "    final_data = torch.from_numpy(np.array(data)).type(dtype)\n",
    "    final_ends = torch.from_numpy(np.array(ends)).type(dtype)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(final_data, final_ends),\n",
    "                                           batch_size=batchsize, shuffle=shuff)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06015e1",
   "metadata": {},
   "source": [
    "# Model Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade1fba",
   "metadata": {},
   "source": [
    "## de Bézenac et al, 2019: CNN with warp mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e569ea",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2632738",
   "metadata": {},
   "source": [
    "The authors used a Charbonnier penality to measure the discrepancy between the predicted next image and the target: \n",
    "\n",
    "$\\rho(x) = (x + \\epsilon)^\\frac{1}{\\alpha}$\n",
    "\n",
    "Note that with $\\epsilon=0$ and $\\alpha=\\frac{1}{2}$, we recover the $\\textit{l}_2$ norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc041ab",
   "metadata": {},
   "source": [
    "Additional penalty terms were added to the loss function, specifically to regulate the divergence of the displacement $\\omega$ between the prediction and the target, its magnitude, and its smoothness:\n",
    "\n",
    "$L_t = \\underset{x \\in \\Omega}\\Sigma \\rho(\\hat{I}_{t+1}(x) - I_{t+1}(x)) + \\lambda_{div}(\\nabla.\\omega_t(x))^2 + \\lambda_{magn}||\\omega_t(x)||^2 + \\lambda_{grad}||\\nabla \\omega_t(x)||^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f28761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regloss(F):\n",
    "    gradient = reduce(np.add, np.gradient(F))\n",
    "    \n",
    "    # TODO: try magnitude regularization in norm loss function\n",
    "    # could also move to inside model class\n",
    "    divergence = np.array([np.mean(gradient.sum(0)**2)])                         # 1st reg term above\n",
    "    magnitude  = np.array([np.mean(np.linalg.norm(F, axis=0, ord=2)**2)])        # 2nd reg term above\n",
    "    smoothness = np.array([np.mean(np.linalg.norm(gradient, axis=0, ord=2)**2)]) # 3rd reg term above\n",
    "    \n",
    "    return torch.from_numpy(divergence).type(torch.FloatTensor), torch.from_numpy(magnitude).type(torch.FloatTensor), torch.from_numpy(smoothness).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741a61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charbonnier(x, alpha, eps):\n",
    "    \n",
    "    res = torch.mean(torch.pow(x + eps, (1. / alpha)))\n",
    "\n",
    "    return res\n",
    "\n",
    "def grad_charbonnier(x, alpha, eps):\n",
    "    \n",
    "    res = (1. / alpha)*torch.pow(x + eps, (1. / alpha) - 1)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108d33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Charbonnier_Loss(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctl, y_pred, y, w, reg, alpha=0.5, eps=0):\n",
    "        ctl.save_for_backward(y_pred, y) # saves Tensor ctl for future call to backward()\n",
    "        \n",
    "        distsq = torch.pow(y_pred - y, 2)\n",
    "        charb_loss = charbonnier(distsq, alpha, eps)\n",
    "        \n",
    "        if reg:\n",
    "            lambda_div, lambda_magn, lambda_grad = 1, -0.1, 0.4\n",
    "            divergence, magnitude, smoothness = compute_regloss(w[0,:,:,:].numpy())\n",
    "            regulariz = lambda_div*divergence + lambda_magn*magnitude + lambda_grad*smoothness\n",
    "            charb_loss += regulariz[0]\n",
    "        \n",
    "        return charb_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    # FIXME: with regularization constants, this is incomplete -> need grad(w) wrt x\n",
    "    def backward(ctl, alpha=0.5, eps=0):\n",
    "        y_pred, y = ctl.saved_tensors\n",
    "        distsq = torch.pow(y_pred - y, 2)\n",
    "\n",
    "        grad = grad_charbonnier(distsq, alpha, eps)\n",
    "        \n",
    "        return grad, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6c0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_loss(y_pred, y): \n",
    "    error = torch.sum((y_pred - y)**2, axis=1)\n",
    "    loss = torch.mean(error)\n",
    "    return loss\n",
    "\n",
    "def charbonnier_loss(y_pred, y, alpha=0.5, epsilon=0):\n",
    "    error = torch.sum((y_pred - y)**2, axis=1)\n",
    "    loss = torch.mean(torch.pow(error + epsilon, (1. / alpha)))\n",
    "    return loss\n",
    "\n",
    "# TODO: add reg terms on W somehow (start with magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406bf74",
   "metadata": {},
   "source": [
    "### Warp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e686d5",
   "metadata": {},
   "source": [
    "The future image is calculated based on a \"warp\" of the motion field estimate, $\\hat{\\omega}$, which can be thought of in this application as the wind vector field:\n",
    "\n",
    "$\\hat{I}_{t+1}(x) = \\underset{y \\in \\Omega} \\Sigma k(x - \\hat{\\omega}(x), y) I_t (y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484aec7",
   "metadata": {},
   "source": [
    "where $k(u, v)$ is a radial basis function kernel, or equivalent a 2D Gaussian probability distribution:\n",
    "\n",
    "$k(x - \\hat{\\omega}(x), y) = \\frac{1}{4 \\pi D\\Delta t}e^{\\frac{-1}{4D \\Delta t} ||x - \\hat{\\omega}(x) - y||^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad59f08",
   "metadata": {},
   "source": [
    "for diffusion coefficient D and time step value $\\Delta t$ between $t$ and $t+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "917ddf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(distsq, D, dt):\n",
    "    \"\"\"Method to implement the k() function or radial basis function kernel \n",
    "    described above.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    distsq : float or torch.FloatTensor : the value of the squared norm of \n",
    "                 the distance\n",
    "    D : float : the diffusion coefficient\n",
    "    dt : float : the timestep\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : float or torch.FloatTensor: the result of the function\n",
    "    \"\"\"\n",
    "    \n",
    "    res = torch.exp(-distsq/(4*D*dt))/(4*np.pi*D*dt)\n",
    "    return res\n",
    "\n",
    "def kernel_gradient(self, dist, D, dt):\n",
    "    \"\"\"Method to implement the gradient of the k() function with respect to \n",
    "    the distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dist : float : the distance\n",
    "    D : float : the diffusion coefficient\n",
    "    dt : float : the timestep\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : float : the gradient of the function with respect to the distance\n",
    "    \"\"\"\n",
    "    \n",
    "    res = dist*torch.exp(-(dist**2).sum(1)/(4*D*dt))/(8*np.pi*D**2*dt**2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d65cd",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b915e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(*models):\n",
    "    for model in models:\n",
    "        for module in model.modules():\n",
    "            \n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d) or isinstance(module, nn.Linear):\n",
    "                \n",
    "                # He initialization, from He, K. et al, 2015\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "                    \n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71dfa15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(_EncoderBlock, self).__init__()\n",
    "        self.cv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.lr1 = nn.LeakyReLU(0.1)\n",
    "        self.cv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.lr2 = nn.LeakyReLU(0.1)\n",
    "        self.maxp = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual=x\n",
    "        #print(\"residual\",residual.shape)\n",
    "        out=self.cv1(x)\n",
    "        #print(\"cv1\",out.shape)\n",
    "        out=self.bn1(out)\n",
    "        #print(\"bn1\",out.shape)\n",
    "        out=self.lr1(out)\n",
    "        #print(\"lr1\",out.shape)\n",
    "        out+=residual\n",
    "        out=self.cv2(out)\n",
    "        out=self.bn2(out)\n",
    "        out=self.lr2(out)\n",
    "        out=self.maxp(out)\n",
    "        return out\n",
    "\n",
    "class _DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(_DecoderBlock, self).__init__()\n",
    "        self.cv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.lr1 = nn.LeakyReLU(0.1)\n",
    "        self.cv2 = nn.Conv2d(in_channels, middle_channels, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(middle_channels) \n",
    "        self.lr2 = nn.LeakyReLU(0.1)\n",
    "        self.tcv = nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual=x\n",
    "        #print(\"residual\",residual.shape)\n",
    "        out=self.cv1(x)\n",
    "        #print(\"cv1\",out.shape)\n",
    "        out=self.bn1(out)\n",
    "        #print(\"bn1\",out.shape)\n",
    "        out=self.lr1(out)\n",
    "        #print(\"lr1\",out.shape)\n",
    "        out+=residual\n",
    "        out=self.cv2(out)\n",
    "        out=self.bn2(out)\n",
    "        out=self.lr2(out)\n",
    "        out=self.tcv(out)\n",
    "        return out\n",
    "\n",
    "class _CenterBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(_CenterBlock, self).__init__()\n",
    "        self.cv1 = nn.Conv2d(in_channels,in_channels , kernel_size=3,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)  \n",
    "        self.lr1 = nn.LeakyReLU(0.1)\n",
    "        self.cv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels) \n",
    "        self.lr2 = nn.LeakyReLU(0.1)\n",
    "        self.tcv = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual=x\n",
    "        out=self.cv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out=self.lr1(out)\n",
    "        out+=residual\n",
    "        out=self.cv2(out)\n",
    "        out=self.bn2(out)\n",
    "        out=self.lr2(out)\n",
    "        out=self.tcv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc45117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDNN(nn.Module):\n",
    "    \"\"\"Class to implement a physics-driven Convolution-Deconvolution Neural \n",
    "    Network (CDNN) as described in (de Bezenac et al, 2019). This model \n",
    "    takes as input historical image(s) of Sea Surface Temperature (SST)\n",
    "    data, X, and uses a convolutional neural network (CNN) to estimate the \n",
    "    wind vector field W that drives the motion of X. From there, the next \n",
    "    image is predicted using a \"warping\" of the most recent input image and W,\n",
    "    as if to see how the SST variable evolves with the wind. \n",
    "    \n",
    "    The \"warping\" in this paper is a radial basis function kernel, or a \n",
    "    Gaussian centered in X-W. Other models we will implement later may use a\n",
    "    different warping scheme. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hist=1):\n",
    "        \"\"\"Function to construct the model. \n",
    "           \n",
    "           Parameters\n",
    "           ----------\n",
    "           hist : int : the number of days of \"history\" to use for prediction, \n",
    "                        default = 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CDNN, self).__init__()\n",
    "        \n",
    "        self.enc1 = _EncoderBlock(hist, 64)  \n",
    "        self.enc2 = _EncoderBlock(64, 128)\n",
    "        self.enc3 = _EncoderBlock(128, 256)\n",
    "        self.enc4 = _EncoderBlock(256, 512)\n",
    "        self.dec4 = _CenterBlock(512, 386)\n",
    "        self.dec3 = _DecoderBlock(386+256, 256, 194)\n",
    "        self.dec2 = _DecoderBlock(194+128, 128, 98)\n",
    "        self.dec1 = _DecoderBlock(98+64, 64, 2)\n",
    "        \n",
    "        self.final = nn.Sequential(nn.Conv2d(2, 2, kernel_size=3),) \n",
    "        initialize_weights(self)\n",
    "\n",
    "        self.hist = hist\n",
    "\n",
    "    def wind(self, x):\n",
    "        \"\"\"Function to estimate the wind vector field from historical input images.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.FloatTensor : the historical input images\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        wind : torch.FloatTensor : the estimated wind vector field\n",
    "        \"\"\"\n",
    "        \n",
    "        enc1 = self.enc1(x)\n",
    "        #print(\"x\",x.shape)\n",
    "        #print(\"enc1\",enc1.shape)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        #print(\"enc2\",enc2.shape)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        #print(\"enc3\",enc3.shape)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        #print(\"enc4\",enc4.shape)\n",
    "        dec4 = self.dec4(enc4)\n",
    "        #print(\"dec4\",dec4.shape)\n",
    "        \n",
    "        dec3 = self.dec3(torch.cat([dec4, F.interpolate(enc3, dec4.size()[2:], mode='bilinear')], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, F.interpolate(enc2, dec3.size()[2:], mode='bilinear')], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, F.interpolate(enc1, dec2.size()[2:], mode='bilinear')], 1))\n",
    "        final = self.final(dec1)\n",
    "        \n",
    "        wind = F.interpolate(final, x.size()[2:], mode='bilinear')\n",
    "\n",
    "        return wind\n",
    "    \n",
    "    @staticmethod\n",
    "    def warp(I, W, hist):\n",
    "        \"\"\"Function to compute the warping of the input data and an estimated \n",
    "        wind vector field, in order to produce an output predicted image. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        I : torch.FloatTensor : the most recent input image to warp\n",
    "        W : torch.FloatTensor : the estimated wind vector field\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        warped : torch.FloatTensor : the warped prediction image\n",
    "        \"\"\"\n",
    "        \n",
    "        D = 0.45\n",
    "        dt = 1\n",
    "        \n",
    "        interval=torch.arange(I.size()[-1]).type(torch.FloatTensor)\n",
    "        \n",
    "        x1 = interval[None,:,None,None,None]\n",
    "        x2 = interval[None,None,:,None,None]\n",
    "        y1 = interval[None,None,None,:,None]\n",
    "        y2 = interval[None,None,None,None,:]\n",
    "        \n",
    "        # x - wind - y\n",
    "        distsq = (x1-y1-W[:,0,:,:,None,None])**2+(x2-y2-W[:,1,:,:,None,None])**2         \n",
    "        mult = I[:, hist-1, None,None,:,:] * kernel(distsq, D, dt)\n",
    "        warped = mult.sum(4).sum(3)\n",
    "        \n",
    "        return warped\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Function to execute the forward pass of the model. All of the \n",
    "        computations are done in the methods above, so this function \n",
    "        simply returns their outputs.\n",
    "        \n",
    "        Note: the wind vector field W is returned for use in the \n",
    "        regularized loss function later. For simple difference loss \n",
    "        functions, returning y_pred is sufficient. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.FloatTensor : the historical input images\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        W : torch.FloatTensor : the estimated wind vector field\n",
    "        y_pred : torch.FloatTensor : the predicted next image\n",
    "        \"\"\"\n",
    "        \n",
    "        W = self.wind(x)\n",
    "        y_pred = self.warp(x, W, self.hist) \n",
    "        \n",
    "        return W, y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd072838",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6550149",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "566d181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    \n",
    "    def __init__(self, name, trainset, valset, testset, model, loss_fn, regloss, test_loss, optimizer, examples, outdir):\n",
    "        self.name = name\n",
    "        self.train_loader = trainset \n",
    "        self.val_loader = valset\n",
    "        self.test_loader = testset\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.regloss = regloss\n",
    "        self.test_loss = test_loss\n",
    "        self.optimizer = optimizer\n",
    "        self.examples = examples\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        self.test_losses  = []\n",
    "        \n",
    "        # Set up a directory for the experiment\n",
    "        self.dir_setup(outdir)\n",
    "        \n",
    "    def dir_setup(self, parent):\n",
    "        self.outdir = parent + \"/\" + self.name\n",
    "        os.mkdir(self.outdir)\n",
    "        print(\"Created new directory to save model states and results: \" + self.outdir)\n",
    "    \n",
    "    def train_loop(self):\n",
    "        size = len(self.train_loader.dataset)\n",
    "        # Set the model to training mode - important for batch normalization and dropout layers\n",
    "        # Unnecessary in this situation but added for best practices\n",
    "        self.model.train()\n",
    "    \n",
    "        losses = []\n",
    "    \n",
    "        for batch, (X, y) in enumerate(self.train_loader):\n",
    "            # Compute prediction and loss\n",
    "            #X = X.to(device) \n",
    "            #y = y.to(device)\n",
    "            outputs = self.model(X)\n",
    "            wind = outputs[0]\n",
    "            y_pred = outputs[1]\n",
    "            \n",
    "            if \"MSE\" or \"Err\" in self.name:\n",
    "                loss = self.loss_fn(y_pred, y)\n",
    "            else:\n",
    "                loss = self.loss_fn(y_pred, y, wind, self.regloss)\n",
    "                \n",
    "            losses.append(loss.item())\n",
    "        \n",
    "            print(\"Step:\", batch, \"Loss:\", loss)\n",
    "        \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    # TODO: implement cross-validation?\n",
    "    # example: https://saturncloud.io/blog/how-to-use-kfold-cross-validation-with-dataloaders-in-pytorch/\n",
    "    def val_loop(self):\n",
    "        # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "        # Unnecessary in this situation but added for best practices\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        num_loops = 0\n",
    "\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.val_loader:\n",
    "                #X = X.to(device) \n",
    "                #y = y.to(device)\n",
    "                outputs = self.model(X)\n",
    "                y_pred = outputs[1]\n",
    "            \n",
    "                step_loss = self.loss_fn(y_pred, y).item()\n",
    "                print(\"Item:\", num_loops, \"Loss:\", step_loss)\n",
    "                losses.append(step_loss)\n",
    "                num_loops += 1\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def test(self):\n",
    "        # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "        # Unnecessary in this situation but added for best practices\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        num_loops = 0\n",
    "\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.test_loader:\n",
    "                #X = X.to(device) \n",
    "                #y = y.to(device)\n",
    "                outputs = self.model(X)\n",
    "                y_pred = outputs[1]\n",
    "            \n",
    "                step_loss = self.test_loss(y_pred, y).item()\n",
    "                print(\"Item:\", num_loops, \"Loss:\", step_loss)\n",
    "                losses.append(step_loss)\n",
    "                num_loops += 1\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def save_results(self, losses, fname):\n",
    "        np.save(fname + \".npy\", losses)\n",
    "    \n",
    "    def save_model_state(self, epoch):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': self.train_losses,\n",
    "            }, self.outdir)\n",
    "        \n",
    "        print(\"Saved checkpoint at epoch\", epoch)\n",
    "    \n",
    "    def run(self, epochs):\n",
    "        print(\"Running experiment: \" + self.name + \"...\")\n",
    "        \n",
    "        # -------------------- Training -------------------------\n",
    "        \n",
    "        print(\"Training over \" + str(epochs) + \" epochs...\")\n",
    "                \n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            \n",
    "            start = datetime.datetime.now()\n",
    "            epoch_losses = self.train_loop()\n",
    "            \n",
    "            fname = self.outdir + \"/train_epoch_\" + str(t)\n",
    "            self.save_results(epoch_losses, fname)\n",
    "            \n",
    "            epoch_mean = np.round(np.mean(epoch_losses), 5)\n",
    "            \n",
    "            print(\"Mean:\", epoch_mean)\n",
    "            print(\"Runtime:\", datetime.datetime.now() - start)\n",
    "            \n",
    "            self.train_losses.append(epoch_mean)\n",
    "            \n",
    "            if t % 100 == 0:\n",
    "                self.save_model_state(t)\n",
    "        \n",
    "        self.plot_loss(self.train_losses, \"Training Loss\", \"Epoch\", [\"train\"])\n",
    "        \n",
    "        # -------------------- Validation ------------------------\n",
    "        self.visualize_examples()\n",
    "\n",
    "        # -------------------- Testing ---------------------------\n",
    "    \n",
    "    # TODO: implement with the \"interesting\" examples found earlier\n",
    "    def visualize_examples(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.examples:\n",
    "                #X = X.to(device) \n",
    "                #y = y.to(device)\n",
    "                outputs = self.model(X)\n",
    "                y_pred = outputs[1]\n",
    "                step_loss = self.test_loss(y_pred, y).item()\n",
    "                \n",
    "                fig, ax = plt.subplots(nrows=1, ncols=2, figzise=(8,8))\n",
    "                ax[0].imshow(y)\n",
    "                ax[0].set_title(\"Ground truth\")\n",
    "                ax[1].imshow(y_pred)\n",
    "                ax[1].set_title(\"Prediction\")\n",
    "                \n",
    "            \n",
    "    def plot_loss(self, loss_to_plot, title, xlab, legend_items):\n",
    "        plt.plot(loss_to_plot)\n",
    "        plt.title(self.name + \" \" + title)\n",
    "        \n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(xlab)\n",
    "        plt.legend(legend_items, loc=\"upper left\")\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c33ed",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14566a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32352\n",
      "0\n",
      "3600\n",
      "322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x153733f286d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "split = train_val_test_split_files(datadir(\"sst_npy/\"), [0.9, 0, 0.1]) # using small amount for now, to test\n",
    "\n",
    "train_split = split[0]\n",
    "val_split   = split[1]\n",
    "test_split  = split[2]\n",
    "\n",
    "# Total number of files in each split\n",
    "print(len(train_split))\n",
    "print(len(val_split))\n",
    "print(len(test_split))\n",
    "\n",
    "# For faster training, try larger batchsize\n",
    "training_loader = create_dataloader(batchsize=100, files=train_split, ndays=4, shuff=True)\n",
    "#val_loader = create_dataloader(batchsize=12, files=val_split, ndays=4, shuff=True)\n",
    "\n",
    "# Total number of batches in the loader\n",
    "print(len(training_loader))\n",
    "\n",
    "train_first, train_next = next(iter(training_loader)) \n",
    "plt.imshow(train_first[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "539b084e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already ran experiments today.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"/projectnb/labci/Lucia/rainfall-pde-ml/experiments/\" + str(datetime.date.today()))\n",
    "except FileExistsError as e:\n",
    "    print(\"Already ran experiments today.\")\n",
    "finally:\n",
    "    path = \"/projectnb/labci/Lucia/rainfall-pde-ml/experiments/\" + str(datetime.date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6c38fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_loader = create_dataloader(batchsize=1, files=os.listdir(datadir(\"sst_npy/\")), \n",
    "                                   ndays=4, shuff=False, all_data=False, regs=[3, 22, 29, 44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bdccd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1537328c9310>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(example_loader))\n",
    "\n",
    "ex_first, ex_next = next(iter(example_loader)) \n",
    "plt.imshow(ex_first[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbe793",
   "metadata": {},
   "source": [
    "### Training with MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec38d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory to save model states and results: /projectnb/labci/Lucia/rainfall-pde-ml/experiments/2023-09-15/Bezenac_MSE_small_3\n",
      "Running experiment: Bezenac_MSE_small_3...\n",
      "Training over 5 epochs...\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Step: 0 Loss: tensor(71.3972, grad_fn=<MseLossBackward0>)\n",
      "Step: 1 Loss: tensor(109.3598, grad_fn=<MseLossBackward0>)\n",
      "Step: 2 Loss: tensor(39.6824, grad_fn=<MseLossBackward0>)\n",
      "Step: 3 Loss: tensor(56.3631, grad_fn=<MseLossBackward0>)\n",
      "Step: 4 Loss: tensor(60.0565, grad_fn=<MseLossBackward0>)\n",
      "Step: 5 Loss: tensor(36.2472, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "net0 = CDNN(hist=4)\n",
    "optim = torch.optim.Adam(net0.parameters(), lr=1e-3)\n",
    "\n",
    "exp0 = Experiment(name=\"Bezenac_MSE_small_3\",                           # de Bezenac model, trained on MSE Loss\n",
    "                  trainset=training_loader, valset=None, testset=None,  # data loaders\n",
    "                  model=net0,                                           # model with 4 days of history\n",
    "                  loss_fn=nn.MSELoss(reduction=\"mean\"),                 # loss function for training\n",
    "                  regloss=False,                                        # whether to regularize the training loss  \n",
    "                  test_loss=nn.MSELoss(reduction=\"mean\"),               # loss function for testing\n",
    "                  optimizer=optim,\n",
    "                  examples=None,\n",
    "                  outdir=path)\n",
    "\n",
    "exp0.run(epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beab998",
   "metadata": {},
   "source": [
    "### Training with (Unregularized) Charbonnier Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "858610d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory to save model states and results: /projectnb/labci/Lucia/rainfall-pde-ml/experiments/2023-09-15/Bezenac_Charb_small_3\n",
      "Running experiment: Bezenac_Charb_small_3...\n",
      "Training over 5 epochs...\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Step: 0 Loss: tensor(18999658., grad_fn=<MeanBackward0>)\n",
      "Step: 1 Loss: tensor(69332480., grad_fn=<MeanBackward0>)\n",
      "Step: 2 Loss: tensor(30322246., grad_fn=<MeanBackward0>)\n",
      "Step: 3 Loss: tensor(19650786., grad_fn=<MeanBackward0>)\n",
      "Step: 4 Loss: tensor(23429334., grad_fn=<MeanBackward0>)\n",
      "Step: 5 Loss: tensor(6356429.5000, grad_fn=<MeanBackward0>)\n",
      "Step: 6 Loss: tensor(17030198., grad_fn=<MeanBackward0>)\n",
      "Step: 7 Loss: tensor(2062093.5000, grad_fn=<MeanBackward0>)\n",
      "Step: 8 Loss: tensor(3158114.7500, grad_fn=<MeanBackward0>)\n",
      "Step: 9 Loss: tensor(1782738.8750, grad_fn=<MeanBackward0>)\n",
      "Step: 10 Loss: tensor(941032.1875, grad_fn=<MeanBackward0>)\n",
      "Step: 11 Loss: tensor(464825.0938, grad_fn=<MeanBackward0>)\n",
      "Step: 12 Loss: tensor(1899194., grad_fn=<MeanBackward0>)\n",
      "Step: 13 Loss: tensor(794010.4375, grad_fn=<MeanBackward0>)\n",
      "Step: 14 Loss: tensor(133711.2969, grad_fn=<MeanBackward0>)\n",
      "Step: 15 Loss: tensor(198392.7500, grad_fn=<MeanBackward0>)\n",
      "Step: 16 Loss: tensor(786341.7500, grad_fn=<MeanBackward0>)\n",
      "Step: 17 Loss: tensor(122468.7500, grad_fn=<MeanBackward0>)\n",
      "Step: 18 Loss: tensor(108619.8750, grad_fn=<MeanBackward0>)\n",
      "Step: 19 Loss: tensor(121458.2266, grad_fn=<MeanBackward0>)\n",
      "Step: 20 Loss: tensor(25420.1738, grad_fn=<MeanBackward0>)\n",
      "Step: 21 Loss: tensor(19359.6562, grad_fn=<MeanBackward0>)\n",
      "Step: 22 Loss: tensor(44792.7227, grad_fn=<MeanBackward0>)\n",
      "Step: 23 Loss: tensor(15348.8164, grad_fn=<MeanBackward0>)\n",
      "Step: 24 Loss: tensor(31842.2578, grad_fn=<MeanBackward0>)\n",
      "Step: 25 Loss: tensor(15863.8154, grad_fn=<MeanBackward0>)\n",
      "Step: 26 Loss: tensor(381939.8438, grad_fn=<MeanBackward0>)\n",
      "Step: 27 Loss: tensor(12168.2461, grad_fn=<MeanBackward0>)\n",
      "Step: 28 Loss: tensor(11657.0303, grad_fn=<MeanBackward0>)\n",
      "Step: 29 Loss: tensor(9091.9658, grad_fn=<MeanBackward0>)\n",
      "Step: 30 Loss: tensor(21531.7285, grad_fn=<MeanBackward0>)\n",
      "Step: 31 Loss: tensor(33084.5039, grad_fn=<MeanBackward0>)\n",
      "Step: 32 Loss: tensor(19796.7637, grad_fn=<MeanBackward0>)\n",
      "Step: 33 Loss: tensor(4435.0474, grad_fn=<MeanBackward0>)\n",
      "Step: 34 Loss: tensor(12349.2471, grad_fn=<MeanBackward0>)\n",
      "Step: 35 Loss: tensor(4575.4663, grad_fn=<MeanBackward0>)\n",
      "Step: 36 Loss: tensor(15992.8994, grad_fn=<MeanBackward0>)\n",
      "Step: 37 Loss: tensor(11640.4014, grad_fn=<MeanBackward0>)\n",
      "Step: 38 Loss: tensor(8902.2705, grad_fn=<MeanBackward0>)\n",
      "Step: 39 Loss: tensor(4447.1572, grad_fn=<MeanBackward0>)\n",
      "Step: 40 Loss: tensor(5850.0430, grad_fn=<MeanBackward0>)\n",
      "Step: 41 Loss: tensor(4693.6558, grad_fn=<MeanBackward0>)\n",
      "Step: 42 Loss: tensor(3926.8713, grad_fn=<MeanBackward0>)\n",
      "Step: 43 Loss: tensor(3842.3232, grad_fn=<MeanBackward0>)\n",
      "Mean: 4509470.13407\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Step: 0 Loss: tensor(8490.2373, grad_fn=<MeanBackward0>)\n",
      "Step: 1 Loss: tensor(2630.4619, grad_fn=<MeanBackward0>)\n",
      "Step: 2 Loss: tensor(3628.1609, grad_fn=<MeanBackward0>)\n",
      "Step: 3 Loss: tensor(4270.6167, grad_fn=<MeanBackward0>)\n",
      "Step: 4 Loss: tensor(5831.4907, grad_fn=<MeanBackward0>)\n",
      "Step: 5 Loss: tensor(7056.4507, grad_fn=<MeanBackward0>)\n",
      "Step: 6 Loss: tensor(2958.9304, grad_fn=<MeanBackward0>)\n",
      "Step: 7 Loss: tensor(4023.6570, grad_fn=<MeanBackward0>)\n",
      "Step: 8 Loss: tensor(2935.2080, grad_fn=<MeanBackward0>)\n",
      "Step: 9 Loss: tensor(2544.2856, grad_fn=<MeanBackward0>)\n",
      "Step: 10 Loss: tensor(10219.6699, grad_fn=<MeanBackward0>)\n",
      "Step: 11 Loss: tensor(1881.4408, grad_fn=<MeanBackward0>)\n",
      "Step: 12 Loss: tensor(2746.9990, grad_fn=<MeanBackward0>)\n",
      "Step: 13 Loss: tensor(3516.3630, grad_fn=<MeanBackward0>)\n",
      "Step: 14 Loss: tensor(3828.0808, grad_fn=<MeanBackward0>)\n",
      "Step: 15 Loss: tensor(1508.9156, grad_fn=<MeanBackward0>)\n",
      "Step: 16 Loss: tensor(1965.8301, grad_fn=<MeanBackward0>)\n",
      "Step: 17 Loss: tensor(3458.2607, grad_fn=<MeanBackward0>)\n",
      "Step: 18 Loss: tensor(2680.2983, grad_fn=<MeanBackward0>)\n",
      "Step: 19 Loss: tensor(741.3386, grad_fn=<MeanBackward0>)\n",
      "Step: 20 Loss: tensor(1948.0781, grad_fn=<MeanBackward0>)\n",
      "Step: 21 Loss: tensor(4300.7490, grad_fn=<MeanBackward0>)\n",
      "Step: 22 Loss: tensor(3389.0752, grad_fn=<MeanBackward0>)\n",
      "Step: 23 Loss: tensor(6383.9414, grad_fn=<MeanBackward0>)\n",
      "Step: 24 Loss: tensor(16037.8096, grad_fn=<MeanBackward0>)\n",
      "Step: 25 Loss: tensor(4061.7273, grad_fn=<MeanBackward0>)\n",
      "Step: 26 Loss: tensor(4889.5684, grad_fn=<MeanBackward0>)\n",
      "Step: 27 Loss: tensor(2182.7961, grad_fn=<MeanBackward0>)\n",
      "Step: 28 Loss: tensor(3637.4102, grad_fn=<MeanBackward0>)\n",
      "Step: 29 Loss: tensor(1298.3373, grad_fn=<MeanBackward0>)\n",
      "Step: 30 Loss: tensor(3567.5840, grad_fn=<MeanBackward0>)\n",
      "Step: 31 Loss: tensor(1831.3246, grad_fn=<MeanBackward0>)\n",
      "Step: 32 Loss: tensor(3095.0811, grad_fn=<MeanBackward0>)\n",
      "Step: 33 Loss: tensor(1297.3842, grad_fn=<MeanBackward0>)\n",
      "Step: 34 Loss: tensor(1341.3975, grad_fn=<MeanBackward0>)\n",
      "Step: 35 Loss: tensor(1451.0104, grad_fn=<MeanBackward0>)\n",
      "Step: 36 Loss: tensor(2012.8008, grad_fn=<MeanBackward0>)\n",
      "Step: 37 Loss: tensor(1328.2373, grad_fn=<MeanBackward0>)\n",
      "Step: 38 Loss: tensor(1381.8970, grad_fn=<MeanBackward0>)\n",
      "Step: 39 Loss: tensor(1542.2538, grad_fn=<MeanBackward0>)\n",
      "Step: 40 Loss: tensor(707.5552, grad_fn=<MeanBackward0>)\n",
      "Step: 41 Loss: tensor(2085.7532, grad_fn=<MeanBackward0>)\n",
      "Step: 42 Loss: tensor(1136.5665, grad_fn=<MeanBackward0>)\n",
      "Step: 43 Loss: tensor(1053.8439, grad_fn=<MeanBackward0>)\n",
      "Mean: 3383.61086\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Step: 0 Loss: tensor(1293.8690, grad_fn=<MeanBackward0>)\n",
      "Step: 1 Loss: tensor(1961.3707, grad_fn=<MeanBackward0>)\n",
      "Step: 2 Loss: tensor(4009.7178, grad_fn=<MeanBackward0>)\n",
      "Step: 3 Loss: tensor(1448.5933, grad_fn=<MeanBackward0>)\n",
      "Step: 4 Loss: tensor(641.8228, grad_fn=<MeanBackward0>)\n",
      "Step: 5 Loss: tensor(2091.5544, grad_fn=<MeanBackward0>)\n",
      "Step: 6 Loss: tensor(1580.4183, grad_fn=<MeanBackward0>)\n",
      "Step: 7 Loss: tensor(924.8819, grad_fn=<MeanBackward0>)\n",
      "Step: 8 Loss: tensor(826.9891, grad_fn=<MeanBackward0>)\n",
      "Step: 9 Loss: tensor(736.5206, grad_fn=<MeanBackward0>)\n",
      "Step: 10 Loss: tensor(1609.3444, grad_fn=<MeanBackward0>)\n",
      "Step: 11 Loss: tensor(2080.9814, grad_fn=<MeanBackward0>)\n",
      "Step: 12 Loss: tensor(1766.8707, grad_fn=<MeanBackward0>)\n",
      "Step: 13 Loss: tensor(1121.3453, grad_fn=<MeanBackward0>)\n",
      "Step: 14 Loss: tensor(1161.1122, grad_fn=<MeanBackward0>)\n",
      "Step: 15 Loss: tensor(2391.3850, grad_fn=<MeanBackward0>)\n",
      "Step: 16 Loss: tensor(2478.9426, grad_fn=<MeanBackward0>)\n",
      "Step: 17 Loss: tensor(1254.5316, grad_fn=<MeanBackward0>)\n",
      "Step: 18 Loss: tensor(2217.5298, grad_fn=<MeanBackward0>)\n",
      "Step: 19 Loss: tensor(1067.6985, grad_fn=<MeanBackward0>)\n",
      "Step: 20 Loss: tensor(1390.6696, grad_fn=<MeanBackward0>)\n",
      "Step: 21 Loss: tensor(2145.5549, grad_fn=<MeanBackward0>)\n",
      "Step: 22 Loss: tensor(730.7078, grad_fn=<MeanBackward0>)\n",
      "Step: 23 Loss: tensor(832.9511, grad_fn=<MeanBackward0>)\n",
      "Step: 24 Loss: tensor(1641.7191, grad_fn=<MeanBackward0>)\n",
      "Step: 25 Loss: tensor(1042.7168, grad_fn=<MeanBackward0>)\n",
      "Step: 26 Loss: tensor(733.7786, grad_fn=<MeanBackward0>)\n",
      "Step: 27 Loss: tensor(1082.0162, grad_fn=<MeanBackward0>)\n",
      "Step: 28 Loss: tensor(1316.7340, grad_fn=<MeanBackward0>)\n",
      "Step: 29 Loss: tensor(1631.9116, grad_fn=<MeanBackward0>)\n",
      "Step: 30 Loss: tensor(1000.4592, grad_fn=<MeanBackward0>)\n",
      "Step: 31 Loss: tensor(927.2758, grad_fn=<MeanBackward0>)\n",
      "Step: 32 Loss: tensor(641.7432, grad_fn=<MeanBackward0>)\n",
      "Step: 33 Loss: tensor(1331.9147, grad_fn=<MeanBackward0>)\n",
      "Step: 34 Loss: tensor(1271.2133, grad_fn=<MeanBackward0>)\n",
      "Step: 35 Loss: tensor(1569.7174, grad_fn=<MeanBackward0>)\n",
      "Step: 36 Loss: tensor(870.4200, grad_fn=<MeanBackward0>)\n",
      "Step: 37 Loss: tensor(621.5682, grad_fn=<MeanBackward0>)\n",
      "Step: 38 Loss: tensor(877.8511, grad_fn=<MeanBackward0>)\n",
      "Step: 39 Loss: tensor(719.4592, grad_fn=<MeanBackward0>)\n",
      "Step: 40 Loss: tensor(510.5116, grad_fn=<MeanBackward0>)\n",
      "Step: 41 Loss: tensor(1270.9576, grad_fn=<MeanBackward0>)\n",
      "Step: 42 Loss: tensor(620.9312, grad_fn=<MeanBackward0>)\n",
      "Step: 43 Loss: tensor(1210.8983, grad_fn=<MeanBackward0>)\n",
      "Mean: 1333.16273\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Step: 0 Loss: tensor(2189.5051, grad_fn=<MeanBackward0>)\n",
      "Step: 1 Loss: tensor(1058.9398, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2 Loss: tensor(1201.5653, grad_fn=<MeanBackward0>)\n",
      "Step: 3 Loss: tensor(594.3157, grad_fn=<MeanBackward0>)\n",
      "Step: 4 Loss: tensor(1371.3224, grad_fn=<MeanBackward0>)\n",
      "Step: 5 Loss: tensor(4455.9126, grad_fn=<MeanBackward0>)\n",
      "Step: 6 Loss: tensor(486.5388, grad_fn=<MeanBackward0>)\n",
      "Step: 7 Loss: tensor(966.7439, grad_fn=<MeanBackward0>)\n",
      "Step: 8 Loss: tensor(839.6509, grad_fn=<MeanBackward0>)\n",
      "Step: 9 Loss: tensor(632.9714, grad_fn=<MeanBackward0>)\n",
      "Step: 10 Loss: tensor(642.0200, grad_fn=<MeanBackward0>)\n",
      "Step: 11 Loss: tensor(1354.6632, grad_fn=<MeanBackward0>)\n",
      "Step: 12 Loss: tensor(1190.2474, grad_fn=<MeanBackward0>)\n",
      "Step: 13 Loss: tensor(537.4678, grad_fn=<MeanBackward0>)\n",
      "Step: 14 Loss: tensor(1703.9889, grad_fn=<MeanBackward0>)\n",
      "Step: 15 Loss: tensor(1787.1489, grad_fn=<MeanBackward0>)\n",
      "Step: 16 Loss: tensor(769.7100, grad_fn=<MeanBackward0>)\n",
      "Step: 17 Loss: tensor(1842.5006, grad_fn=<MeanBackward0>)\n",
      "Step: 18 Loss: tensor(448.9977, grad_fn=<MeanBackward0>)\n",
      "Step: 19 Loss: tensor(884.3109, grad_fn=<MeanBackward0>)\n",
      "Step: 20 Loss: tensor(547.8301, grad_fn=<MeanBackward0>)\n",
      "Step: 21 Loss: tensor(567.6872, grad_fn=<MeanBackward0>)\n",
      "Step: 22 Loss: tensor(517.6945, grad_fn=<MeanBackward0>)\n",
      "Step: 23 Loss: tensor(592.5685, grad_fn=<MeanBackward0>)\n",
      "Step: 24 Loss: tensor(1069.6830, grad_fn=<MeanBackward0>)\n",
      "Step: 25 Loss: tensor(727.3040, grad_fn=<MeanBackward0>)\n",
      "Step: 26 Loss: tensor(1363.9569, grad_fn=<MeanBackward0>)\n",
      "Step: 27 Loss: tensor(1552.7338, grad_fn=<MeanBackward0>)\n",
      "Step: 28 Loss: tensor(481.5448, grad_fn=<MeanBackward0>)\n",
      "Step: 29 Loss: tensor(1784.5674, grad_fn=<MeanBackward0>)\n",
      "Step: 30 Loss: tensor(675.4666, grad_fn=<MeanBackward0>)\n",
      "Step: 31 Loss: tensor(573.6253, grad_fn=<MeanBackward0>)\n",
      "Step: 32 Loss: tensor(577.6996, grad_fn=<MeanBackward0>)\n",
      "Step: 33 Loss: tensor(478.9565, grad_fn=<MeanBackward0>)\n",
      "Step: 34 Loss: tensor(666.0991, grad_fn=<MeanBackward0>)\n",
      "Step: 35 Loss: tensor(435.1952, grad_fn=<MeanBackward0>)\n",
      "Step: 36 Loss: tensor(630.2636, grad_fn=<MeanBackward0>)\n",
      "Step: 37 Loss: tensor(809.5840, grad_fn=<MeanBackward0>)\n",
      "Step: 38 Loss: tensor(1017.4251, grad_fn=<MeanBackward0>)\n",
      "Step: 39 Loss: tensor(711.8705, grad_fn=<MeanBackward0>)\n",
      "Step: 40 Loss: tensor(1837.7148, grad_fn=<MeanBackward0>)\n",
      "Step: 41 Loss: tensor(855.2025, grad_fn=<MeanBackward0>)\n",
      "Step: 42 Loss: tensor(950.1176, grad_fn=<MeanBackward0>)\n",
      "Step: 43 Loss: tensor(510.8481, grad_fn=<MeanBackward0>)\n",
      "Mean: 1020.32182\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Step: 0 Loss: tensor(572.1411, grad_fn=<MeanBackward0>)\n",
      "Step: 1 Loss: tensor(571.5398, grad_fn=<MeanBackward0>)\n",
      "Step: 2 Loss: tensor(511.0272, grad_fn=<MeanBackward0>)\n",
      "Step: 3 Loss: tensor(1790.8485, grad_fn=<MeanBackward0>)\n",
      "Step: 4 Loss: tensor(347.9462, grad_fn=<MeanBackward0>)\n",
      "Step: 5 Loss: tensor(600.0360, grad_fn=<MeanBackward0>)\n",
      "Step: 6 Loss: tensor(430.9974, grad_fn=<MeanBackward0>)\n",
      "Step: 7 Loss: tensor(468.6077, grad_fn=<MeanBackward0>)\n",
      "Step: 8 Loss: tensor(707.5261, grad_fn=<MeanBackward0>)\n",
      "Step: 9 Loss: tensor(542.1511, grad_fn=<MeanBackward0>)\n",
      "Step: 10 Loss: tensor(442.9612, grad_fn=<MeanBackward0>)\n",
      "Step: 11 Loss: tensor(902.2246, grad_fn=<MeanBackward0>)\n",
      "Step: 12 Loss: tensor(335.0422, grad_fn=<MeanBackward0>)\n",
      "Step: 13 Loss: tensor(486.7861, grad_fn=<MeanBackward0>)\n",
      "Step: 14 Loss: tensor(840.7521, grad_fn=<MeanBackward0>)\n",
      "Step: 15 Loss: tensor(629.7921, grad_fn=<MeanBackward0>)\n",
      "Step: 16 Loss: tensor(292.8291, grad_fn=<MeanBackward0>)\n",
      "Step: 17 Loss: tensor(570.4711, grad_fn=<MeanBackward0>)\n",
      "Step: 18 Loss: tensor(378.9613, grad_fn=<MeanBackward0>)\n",
      "Step: 19 Loss: tensor(900.9687, grad_fn=<MeanBackward0>)\n",
      "Step: 20 Loss: tensor(474.9761, grad_fn=<MeanBackward0>)\n",
      "Step: 21 Loss: tensor(1162.1273, grad_fn=<MeanBackward0>)\n",
      "Step: 22 Loss: tensor(570.8642, grad_fn=<MeanBackward0>)\n",
      "Step: 23 Loss: tensor(316.7260, grad_fn=<MeanBackward0>)\n",
      "Step: 24 Loss: tensor(935.8864, grad_fn=<MeanBackward0>)\n",
      "Step: 25 Loss: tensor(526.8994, grad_fn=<MeanBackward0>)\n",
      "Step: 26 Loss: tensor(475.0525, grad_fn=<MeanBackward0>)\n",
      "Step: 27 Loss: tensor(682.2429, grad_fn=<MeanBackward0>)\n",
      "Step: 28 Loss: tensor(371.4360, grad_fn=<MeanBackward0>)\n",
      "Step: 29 Loss: tensor(496.1714, grad_fn=<MeanBackward0>)\n",
      "Step: 30 Loss: tensor(1258.9354, grad_fn=<MeanBackward0>)\n",
      "Step: 31 Loss: tensor(404.4240, grad_fn=<MeanBackward0>)\n",
      "Step: 32 Loss: tensor(343.1864, grad_fn=<MeanBackward0>)\n",
      "Step: 33 Loss: tensor(652.6904, grad_fn=<MeanBackward0>)\n",
      "Step: 34 Loss: tensor(382.8551, grad_fn=<MeanBackward0>)\n",
      "Step: 35 Loss: tensor(2517.7734, grad_fn=<MeanBackward0>)\n",
      "Step: 36 Loss: tensor(467.1041, grad_fn=<MeanBackward0>)\n",
      "Step: 37 Loss: tensor(607.3420, grad_fn=<MeanBackward0>)\n",
      "Step: 38 Loss: tensor(847.9194, grad_fn=<MeanBackward0>)\n",
      "Step: 39 Loss: tensor(1417.8185, grad_fn=<MeanBackward0>)\n",
      "Step: 40 Loss: tensor(294.3730, grad_fn=<MeanBackward0>)\n",
      "Step: 41 Loss: tensor(957.8049, grad_fn=<MeanBackward0>)\n",
      "Step: 42 Loss: tensor(434.8357, grad_fn=<MeanBackward0>)\n",
      "Step: 43 Loss: tensor(366.6584, grad_fn=<MeanBackward0>)\n",
      "Mean: 665.67528\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net1 = CDNN(hist=4)\n",
    "optim = torch.optim.Adam(net1.parameters(), lr=1e-3)\n",
    "\n",
    "exp1 = Experiment(name=\"Bezenac_Charb_small_3\",                         # de Bezenac model, trained on Charb Loss\n",
    "                  trainset=training_loader, valset=None, testset=None,  # data loaders\n",
    "                  model=net1,                                           # model with 4 days of history\n",
    "                  loss_fn=charbonnier_loss,                             # loss function for training\n",
    "                  regloss=False,                                        # whether to regularize the training loss \n",
    "                  test_loss=nn.MSELoss(reduction=\"mean\"),               # loss function for testing\n",
    "                  optimizer=optim,\n",
    "                  examples=None,\n",
    "                  outdir=path)\n",
    "\n",
    "exp1.run(epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1ca72",
   "metadata": {},
   "source": [
    "### Training with (Regularized) Charbonnier Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f09f43a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory to save model states and results: /projectnb/labci/Lucia/rainfall-pde-ml/experiments/2023-09-15/Bezenac_CharbReg_small_2\n",
      "Running experiment: Bezenac_CharbReg_small_2...\n",
      "Training over 5 epochs...\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'w' and 'reg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      4\u001b[0m exp2 \u001b[38;5;241m=\u001b[39m Experiment(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBezenac_CharbReg_small_2\u001b[39m\u001b[38;5;124m\"\u001b[39m,                      \u001b[38;5;66;03m# de Bezenac model, trained on Regularized Charb Loss\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                   trainset\u001b[38;5;241m=\u001b[39mtraining_loader, valset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, testset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# data loaders\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                   model\u001b[38;5;241m=\u001b[39mnet2,                                           \u001b[38;5;66;03m# model with 4 days of history\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m                   examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                   outdir\u001b[38;5;241m=\u001b[39mpath)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mexp2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 122\u001b[0m, in \u001b[0;36mExperiment.run\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m     epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutdir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train_epoch_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(t)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_results(epoch_losses, fname)\n",
      "Cell \u001b[0;32mIn[36], line 44\u001b[0m, in \u001b[0;36mExperiment.train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname:\n\u001b[0;32m---> 44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y_pred, y, wind, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregloss)\n",
      "File \u001b[0;32m/projectnb/labci/luciav/.conda/envs/bez-torch/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'w' and 'reg'"
     ]
    }
   ],
   "source": [
    "net2 = CDNN(hist=4)\n",
    "optim = torch.optim.Adam(net2.parameters(), lr=1e-3)\n",
    "\n",
    "exp2 = Experiment(name=\"Bezenac_CharbReg_small_2\",                      # de Bezenac model, trained on Regularized Charb Loss\n",
    "                  trainset=training_loader, valset=None, testset=None,  # data loaders\n",
    "                  model=net2,                                           # model with 4 days of history\n",
    "                  loss_fn=Charbonnier_Loss.apply,                       # loss function for training\n",
    "                  regloss=True,                                         # whether to regularize the training loss \n",
    "                  test_loss=nn.MSELoss(reduction=\"mean\"),               # loss function for testing\n",
    "                  optimizer=optim,\n",
    "                  examples=None,\n",
    "                  outdir=path)\n",
    "\n",
    "exp2.run(epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8b067",
   "metadata": {},
   "source": [
    "### Training with Squared Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8dbbcb5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory to save model states and results: /projectnb/labci/Lucia/rainfall-pde-ml/experiments/2023-09-18/Bezenac_SqErr_full_0\n",
      "Running experiment: Bezenac_SqErr_full_0...\n",
      "Training over 100 epochs...\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Step: 0 Loss: tensor(2090.9492, grad_fn=<MeanBackward0>)\n",
      "Step: 1 Loss: tensor(2365.4634, grad_fn=<MeanBackward0>)\n",
      "Step: 2 Loss: tensor(471.0805, grad_fn=<MeanBackward0>)\n",
      "Step: 3 Loss: tensor(704.5144, grad_fn=<MeanBackward0>)\n",
      "Step: 4 Loss: tensor(246.5536, grad_fn=<MeanBackward0>)\n",
      "Step: 5 Loss: tensor(12.9832, grad_fn=<MeanBackward0>)\n",
      "Step: 6 Loss: tensor(10.6362, grad_fn=<MeanBackward0>)\n",
      "Step: 7 Loss: tensor(13.8139, grad_fn=<MeanBackward0>)\n",
      "Step: 8 Loss: tensor(9.5255, grad_fn=<MeanBackward0>)\n",
      "Step: 9 Loss: tensor(3.9989, grad_fn=<MeanBackward0>)\n",
      "Step: 10 Loss: tensor(18.9895, grad_fn=<MeanBackward0>)\n",
      "Step: 11 Loss: tensor(4.4807, grad_fn=<MeanBackward0>)\n",
      "Step: 12 Loss: tensor(16.5630, grad_fn=<MeanBackward0>)\n",
      "Step: 13 Loss: tensor(3.3242, grad_fn=<MeanBackward0>)\n",
      "Step: 14 Loss: tensor(3.6008, grad_fn=<MeanBackward0>)\n",
      "Step: 15 Loss: tensor(2.3770, grad_fn=<MeanBackward0>)\n",
      "Step: 16 Loss: tensor(2.2871, grad_fn=<MeanBackward0>)\n",
      "Step: 17 Loss: tensor(2.3213, grad_fn=<MeanBackward0>)\n",
      "Step: 18 Loss: tensor(2.0256, grad_fn=<MeanBackward0>)\n",
      "Step: 19 Loss: tensor(2.3268, grad_fn=<MeanBackward0>)\n",
      "Step: 20 Loss: tensor(1.6830, grad_fn=<MeanBackward0>)\n",
      "Step: 21 Loss: tensor(2.0750, grad_fn=<MeanBackward0>)\n",
      "Step: 22 Loss: tensor(2.0544, grad_fn=<MeanBackward0>)\n",
      "Step: 23 Loss: tensor(1.7887, grad_fn=<MeanBackward0>)\n",
      "Step: 24 Loss: tensor(2.9651, grad_fn=<MeanBackward0>)\n",
      "Step: 25 Loss: tensor(1.8308, grad_fn=<MeanBackward0>)\n",
      "Step: 26 Loss: tensor(1.6051, grad_fn=<MeanBackward0>)\n",
      "Step: 27 Loss: tensor(1.7594, grad_fn=<MeanBackward0>)\n",
      "Step: 28 Loss: tensor(1.8739, grad_fn=<MeanBackward0>)\n",
      "Step: 29 Loss: tensor(2.2788, grad_fn=<MeanBackward0>)\n",
      "Step: 30 Loss: tensor(1.5514, grad_fn=<MeanBackward0>)\n",
      "Step: 31 Loss: tensor(2.2036, grad_fn=<MeanBackward0>)\n",
      "Step: 32 Loss: tensor(1.4468, grad_fn=<MeanBackward0>)\n",
      "Step: 33 Loss: tensor(2.0890, grad_fn=<MeanBackward0>)\n",
      "Step: 34 Loss: tensor(1.5341, grad_fn=<MeanBackward0>)\n",
      "Step: 35 Loss: tensor(1.9402, grad_fn=<MeanBackward0>)\n",
      "Step: 36 Loss: tensor(1.3531, grad_fn=<MeanBackward0>)\n",
      "Step: 37 Loss: tensor(1.9240, grad_fn=<MeanBackward0>)\n",
      "Step: 38 Loss: tensor(1.5467, grad_fn=<MeanBackward0>)\n",
      "Step: 39 Loss: tensor(1.8401, grad_fn=<MeanBackward0>)\n",
      "Step: 40 Loss: tensor(1.6801, grad_fn=<MeanBackward0>)\n",
      "Step: 41 Loss: tensor(1.5436, grad_fn=<MeanBackward0>)\n",
      "Step: 42 Loss: tensor(1.7294, grad_fn=<MeanBackward0>)\n",
      "Step: 43 Loss: tensor(1.9617, grad_fn=<MeanBackward0>)\n",
      "Step: 44 Loss: tensor(1.9489, grad_fn=<MeanBackward0>)\n",
      "Step: 45 Loss: tensor(1.8288, grad_fn=<MeanBackward0>)\n",
      "Step: 46 Loss: tensor(1.9401, grad_fn=<MeanBackward0>)\n",
      "Step: 47 Loss: tensor(1.6517, grad_fn=<MeanBackward0>)\n",
      "Step: 48 Loss: tensor(1.3442, grad_fn=<MeanBackward0>)\n",
      "Step: 49 Loss: tensor(1.9515, grad_fn=<MeanBackward0>)\n",
      "Step: 50 Loss: tensor(1.9783, grad_fn=<MeanBackward0>)\n",
      "Step: 51 Loss: tensor(2.1719, grad_fn=<MeanBackward0>)\n",
      "Step: 52 Loss: tensor(2.8044, grad_fn=<MeanBackward0>)\n",
      "Step: 53 Loss: tensor(1.9763, grad_fn=<MeanBackward0>)\n",
      "Step: 54 Loss: tensor(1.3803, grad_fn=<MeanBackward0>)\n",
      "Step: 55 Loss: tensor(2.0006, grad_fn=<MeanBackward0>)\n",
      "Step: 56 Loss: tensor(1.8064, grad_fn=<MeanBackward0>)\n",
      "Step: 57 Loss: tensor(2.1977, grad_fn=<MeanBackward0>)\n",
      "Step: 58 Loss: tensor(1.7413, grad_fn=<MeanBackward0>)\n",
      "Step: 59 Loss: tensor(1.8800, grad_fn=<MeanBackward0>)\n",
      "Step: 60 Loss: tensor(1.9268, grad_fn=<MeanBackward0>)\n",
      "Step: 61 Loss: tensor(1.5776, grad_fn=<MeanBackward0>)\n",
      "Step: 62 Loss: tensor(1.2747, grad_fn=<MeanBackward0>)\n",
      "Step: 63 Loss: tensor(1.6470, grad_fn=<MeanBackward0>)\n",
      "Step: 64 Loss: tensor(1.7838, grad_fn=<MeanBackward0>)\n",
      "Step: 65 Loss: tensor(1.4248, grad_fn=<MeanBackward0>)\n",
      "Step: 66 Loss: tensor(1.6030, grad_fn=<MeanBackward0>)\n",
      "Step: 67 Loss: tensor(1.7885, grad_fn=<MeanBackward0>)\n",
      "Step: 68 Loss: tensor(1.4672, grad_fn=<MeanBackward0>)\n",
      "Step: 69 Loss: tensor(2.1053, grad_fn=<MeanBackward0>)\n",
      "Step: 70 Loss: tensor(1.8803, grad_fn=<MeanBackward0>)\n",
      "Step: 71 Loss: tensor(1.6508, grad_fn=<MeanBackward0>)\n",
      "Step: 72 Loss: tensor(1.6876, grad_fn=<MeanBackward0>)\n",
      "Step: 73 Loss: tensor(1.5550, grad_fn=<MeanBackward0>)\n",
      "Step: 74 Loss: tensor(2.1578, grad_fn=<MeanBackward0>)\n",
      "Step: 75 Loss: tensor(1.6021, grad_fn=<MeanBackward0>)\n",
      "Step: 76 Loss: tensor(1.9459, grad_fn=<MeanBackward0>)\n",
      "Step: 77 Loss: tensor(1.5275, grad_fn=<MeanBackward0>)\n",
      "Step: 78 Loss: tensor(1.6032, grad_fn=<MeanBackward0>)\n",
      "Step: 79 Loss: tensor(1.9420, grad_fn=<MeanBackward0>)\n",
      "Step: 80 Loss: tensor(1.6365, grad_fn=<MeanBackward0>)\n",
      "Step: 81 Loss: tensor(1.9634, grad_fn=<MeanBackward0>)\n",
      "Step: 82 Loss: tensor(1.7230, grad_fn=<MeanBackward0>)\n",
      "Step: 83 Loss: tensor(1.1895, grad_fn=<MeanBackward0>)\n",
      "Step: 84 Loss: tensor(1.4281, grad_fn=<MeanBackward0>)\n",
      "Step: 86 Loss: tensor(1.4231, grad_fn=<MeanBackward0>)\n",
      "Step: 87 Loss: tensor(1.8251, grad_fn=<MeanBackward0>)\n",
      "Step: 88 Loss: tensor(1.4543, grad_fn=<MeanBackward0>)\n",
      "Step: 89 Loss: tensor(1.8673, grad_fn=<MeanBackward0>)\n",
      "Step: 90 Loss: tensor(1.6628, grad_fn=<MeanBackward0>)\n",
      "Step: 91 Loss: tensor(1.5852, grad_fn=<MeanBackward0>)\n",
      "Step: 92 Loss: tensor(1.5552, grad_fn=<MeanBackward0>)\n",
      "Step: 93 Loss: tensor(2.1049, grad_fn=<MeanBackward0>)\n",
      "Step: 94 Loss: tensor(1.5776, grad_fn=<MeanBackward0>)\n",
      "Step: 95 Loss: tensor(1.3888, grad_fn=<MeanBackward0>)\n",
      "Step: 96 Loss: tensor(1.5233, grad_fn=<MeanBackward0>)\n",
      "Step: 97 Loss: tensor(1.5894, grad_fn=<MeanBackward0>)\n",
      "Step: 98 Loss: tensor(1.5015, grad_fn=<MeanBackward0>)\n",
      "Step: 99 Loss: tensor(1.7618, grad_fn=<MeanBackward0>)\n",
      "Step: 100 Loss: tensor(1.9700, grad_fn=<MeanBackward0>)\n",
      "Step: 101 Loss: tensor(1.4827, grad_fn=<MeanBackward0>)\n",
      "Step: 102 Loss: tensor(1.3941, grad_fn=<MeanBackward0>)\n",
      "Step: 103 Loss: tensor(1.7052, grad_fn=<MeanBackward0>)\n",
      "Step: 104 Loss: tensor(1.6283, grad_fn=<MeanBackward0>)\n",
      "Step: 105 Loss: tensor(1.7640, grad_fn=<MeanBackward0>)\n",
      "Step: 106 Loss: tensor(1.7400, grad_fn=<MeanBackward0>)\n",
      "Step: 107 Loss: tensor(1.6233, grad_fn=<MeanBackward0>)\n",
      "Step: 108 Loss: tensor(2.0298, grad_fn=<MeanBackward0>)\n",
      "Step: 109 Loss: tensor(1.6401, grad_fn=<MeanBackward0>)\n",
      "Step: 110 Loss: tensor(1.4841, grad_fn=<MeanBackward0>)\n",
      "Step: 111 Loss: tensor(1.4204, grad_fn=<MeanBackward0>)\n",
      "Step: 112 Loss: tensor(2.1306, grad_fn=<MeanBackward0>)\n",
      "Step: 113 Loss: tensor(1.4994, grad_fn=<MeanBackward0>)\n",
      "Step: 114 Loss: tensor(1.6841, grad_fn=<MeanBackward0>)\n",
      "Step: 115 Loss: tensor(1.5581, grad_fn=<MeanBackward0>)\n",
      "Step: 116 Loss: tensor(1.7381, grad_fn=<MeanBackward0>)\n",
      "Step: 117 Loss: tensor(1.4147, grad_fn=<MeanBackward0>)\n",
      "Step: 118 Loss: tensor(1.7932, grad_fn=<MeanBackward0>)\n",
      "Step: 119 Loss: tensor(1.5476, grad_fn=<MeanBackward0>)\n",
      "Step: 120 Loss: tensor(1.7630, grad_fn=<MeanBackward0>)\n",
      "Step: 121 Loss: tensor(1.5602, grad_fn=<MeanBackward0>)\n",
      "Step: 122 Loss: tensor(1.1822, grad_fn=<MeanBackward0>)\n",
      "Step: 123 Loss: tensor(1.6670, grad_fn=<MeanBackward0>)\n",
      "Step: 124 Loss: tensor(1.3742, grad_fn=<MeanBackward0>)\n",
      "Step: 125 Loss: tensor(1.4882, grad_fn=<MeanBackward0>)\n",
      "Step: 126 Loss: tensor(1.4522, grad_fn=<MeanBackward0>)\n",
      "Step: 127 Loss: tensor(1.6453, grad_fn=<MeanBackward0>)\n",
      "Step: 128 Loss: tensor(1.3407, grad_fn=<MeanBackward0>)\n",
      "Step: 129 Loss: tensor(1.4231, grad_fn=<MeanBackward0>)\n",
      "Step: 130 Loss: tensor(1.4902, grad_fn=<MeanBackward0>)\n",
      "Step: 131 Loss: tensor(1.3359, grad_fn=<MeanBackward0>)\n",
      "Step: 132 Loss: tensor(1.2534, grad_fn=<MeanBackward0>)\n",
      "Step: 133 Loss: tensor(1.2111, grad_fn=<MeanBackward0>)\n",
      "Step: 134 Loss: tensor(1.3174, grad_fn=<MeanBackward0>)\n",
      "Step: 135 Loss: tensor(1.1771, grad_fn=<MeanBackward0>)\n",
      "Step: 136 Loss: tensor(1.4584, grad_fn=<MeanBackward0>)\n",
      "Step: 137 Loss: tensor(1.5225, grad_fn=<MeanBackward0>)\n",
      "Step: 138 Loss: tensor(1.8428, grad_fn=<MeanBackward0>)\n",
      "Step: 139 Loss: tensor(1.2553, grad_fn=<MeanBackward0>)\n",
      "Step: 140 Loss: tensor(1.2764, grad_fn=<MeanBackward0>)\n",
      "Step: 141 Loss: tensor(1.6722, grad_fn=<MeanBackward0>)\n",
      "Step: 142 Loss: tensor(1.1427, grad_fn=<MeanBackward0>)\n",
      "Step: 143 Loss: tensor(1.9129, grad_fn=<MeanBackward0>)\n",
      "Step: 144 Loss: tensor(1.5307, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 145 Loss: tensor(1.4641, grad_fn=<MeanBackward0>)\n",
      "Step: 146 Loss: tensor(1.4222, grad_fn=<MeanBackward0>)\n",
      "Step: 147 Loss: tensor(1.4399, grad_fn=<MeanBackward0>)\n",
      "Step: 148 Loss: tensor(1.5249, grad_fn=<MeanBackward0>)\n",
      "Step: 149 Loss: tensor(1.7260, grad_fn=<MeanBackward0>)\n",
      "Step: 150 Loss: tensor(2.0774, grad_fn=<MeanBackward0>)\n",
      "Step: 151 Loss: tensor(1.5408, grad_fn=<MeanBackward0>)\n",
      "Step: 152 Loss: tensor(1.6125, grad_fn=<MeanBackward0>)\n",
      "Step: 153 Loss: tensor(1.5593, grad_fn=<MeanBackward0>)\n",
      "Step: 154 Loss: tensor(1.6952, grad_fn=<MeanBackward0>)\n",
      "Step: 155 Loss: tensor(1.2752, grad_fn=<MeanBackward0>)\n",
      "Step: 156 Loss: tensor(1.4015, grad_fn=<MeanBackward0>)\n",
      "Step: 157 Loss: tensor(1.7803, grad_fn=<MeanBackward0>)\n",
      "Step: 158 Loss: tensor(1.9871, grad_fn=<MeanBackward0>)\n",
      "Step: 159 Loss: tensor(1.4756, grad_fn=<MeanBackward0>)\n",
      "Step: 160 Loss: tensor(2.1796, grad_fn=<MeanBackward0>)\n",
      "Step: 161 Loss: tensor(1.4655, grad_fn=<MeanBackward0>)\n",
      "Step: 162 Loss: tensor(1.8310, grad_fn=<MeanBackward0>)\n",
      "Step: 163 Loss: tensor(1.4930, grad_fn=<MeanBackward0>)\n",
      "Step: 164 Loss: tensor(1.8027, grad_fn=<MeanBackward0>)\n",
      "Step: 165 Loss: tensor(1.4722, grad_fn=<MeanBackward0>)\n",
      "Step: 166 Loss: tensor(1.6488, grad_fn=<MeanBackward0>)\n",
      "Step: 167 Loss: tensor(1.8236, grad_fn=<MeanBackward0>)\n",
      "Step: 168 Loss: tensor(1.5496, grad_fn=<MeanBackward0>)\n",
      "Step: 169 Loss: tensor(1.5560, grad_fn=<MeanBackward0>)\n",
      "Step: 170 Loss: tensor(1.3032, grad_fn=<MeanBackward0>)\n",
      "Step: 171 Loss: tensor(1.6501, grad_fn=<MeanBackward0>)\n",
      "Step: 172 Loss: tensor(1.6003, grad_fn=<MeanBackward0>)\n",
      "Step: 173 Loss: tensor(1.4915, grad_fn=<MeanBackward0>)\n",
      "Step: 174 Loss: tensor(1.2669, grad_fn=<MeanBackward0>)\n",
      "Step: 175 Loss: tensor(1.2255, grad_fn=<MeanBackward0>)\n",
      "Step: 176 Loss: tensor(1.8634, grad_fn=<MeanBackward0>)\n",
      "Step: 177 Loss: tensor(1.9392, grad_fn=<MeanBackward0>)\n",
      "Step: 178 Loss: tensor(1.3186, grad_fn=<MeanBackward0>)\n",
      "Step: 179 Loss: tensor(1.7631, grad_fn=<MeanBackward0>)\n",
      "Step: 180 Loss: tensor(1.3960, grad_fn=<MeanBackward0>)\n",
      "Step: 181 Loss: tensor(1.6417, grad_fn=<MeanBackward0>)\n",
      "Step: 182 Loss: tensor(1.6549, grad_fn=<MeanBackward0>)\n",
      "Step: 183 Loss: tensor(1.7898, grad_fn=<MeanBackward0>)\n",
      "Step: 184 Loss: tensor(2.2795, grad_fn=<MeanBackward0>)\n",
      "Step: 185 Loss: tensor(1.7389, grad_fn=<MeanBackward0>)\n",
      "Step: 186 Loss: tensor(1.1248, grad_fn=<MeanBackward0>)\n",
      "Step: 187 Loss: tensor(1.6136, grad_fn=<MeanBackward0>)\n",
      "Step: 188 Loss: tensor(2.0392, grad_fn=<MeanBackward0>)\n",
      "Step: 189 Loss: tensor(1.3866, grad_fn=<MeanBackward0>)\n",
      "Step: 190 Loss: tensor(1.7795, grad_fn=<MeanBackward0>)\n",
      "Step: 191 Loss: tensor(1.2825, grad_fn=<MeanBackward0>)\n",
      "Step: 192 Loss: tensor(1.8177, grad_fn=<MeanBackward0>)\n",
      "Step: 193 Loss: tensor(1.4253, grad_fn=<MeanBackward0>)\n",
      "Step: 194 Loss: tensor(1.7276, grad_fn=<MeanBackward0>)\n",
      "Step: 195 Loss: tensor(1.4280, grad_fn=<MeanBackward0>)\n",
      "Step: 196 Loss: tensor(1.7923, grad_fn=<MeanBackward0>)\n",
      "Step: 197 Loss: tensor(1.4152, grad_fn=<MeanBackward0>)\n",
      "Step: 198 Loss: tensor(1.4656, grad_fn=<MeanBackward0>)\n",
      "Step: 199 Loss: tensor(1.5638, grad_fn=<MeanBackward0>)\n",
      "Step: 200 Loss: tensor(1.3648, grad_fn=<MeanBackward0>)\n",
      "Step: 201 Loss: tensor(1.5980, grad_fn=<MeanBackward0>)\n",
      "Step: 202 Loss: tensor(1.4878, grad_fn=<MeanBackward0>)\n",
      "Step: 203 Loss: tensor(1.4867, grad_fn=<MeanBackward0>)\n",
      "Step: 204 Loss: tensor(1.4581, grad_fn=<MeanBackward0>)\n",
      "Step: 205 Loss: tensor(1.5586, grad_fn=<MeanBackward0>)\n",
      "Step: 206 Loss: tensor(1.8810, grad_fn=<MeanBackward0>)\n",
      "Step: 207 Loss: tensor(1.8811, grad_fn=<MeanBackward0>)\n",
      "Step: 208 Loss: tensor(1.4536, grad_fn=<MeanBackward0>)\n",
      "Step: 209 Loss: tensor(1.7992, grad_fn=<MeanBackward0>)\n",
      "Step: 210 Loss: tensor(1.8490, grad_fn=<MeanBackward0>)\n",
      "Step: 211 Loss: tensor(1.3595, grad_fn=<MeanBackward0>)\n",
      "Step: 212 Loss: tensor(1.6356, grad_fn=<MeanBackward0>)\n",
      "Step: 213 Loss: tensor(1.3620, grad_fn=<MeanBackward0>)\n",
      "Step: 214 Loss: tensor(2.2226, grad_fn=<MeanBackward0>)\n",
      "Step: 215 Loss: tensor(1.4507, grad_fn=<MeanBackward0>)\n",
      "Step: 216 Loss: tensor(1.5749, grad_fn=<MeanBackward0>)\n",
      "Step: 217 Loss: tensor(1.4970, grad_fn=<MeanBackward0>)\n",
      "Step: 218 Loss: tensor(2.2279, grad_fn=<MeanBackward0>)\n",
      "Step: 219 Loss: tensor(1.0939, grad_fn=<MeanBackward0>)\n",
      "Step: 220 Loss: tensor(1.5052, grad_fn=<MeanBackward0>)\n",
      "Step: 221 Loss: tensor(1.6475, grad_fn=<MeanBackward0>)\n",
      "Step: 222 Loss: tensor(1.5319, grad_fn=<MeanBackward0>)\n",
      "Step: 223 Loss: tensor(1.5803, grad_fn=<MeanBackward0>)\n",
      "Step: 224 Loss: tensor(1.6257, grad_fn=<MeanBackward0>)\n",
      "Step: 225 Loss: tensor(1.1854, grad_fn=<MeanBackward0>)\n",
      "Step: 226 Loss: tensor(1.1153, grad_fn=<MeanBackward0>)\n",
      "Step: 227 Loss: tensor(2.1017, grad_fn=<MeanBackward0>)\n",
      "Step: 228 Loss: tensor(1.5616, grad_fn=<MeanBackward0>)\n",
      "Step: 229 Loss: tensor(1.3710, grad_fn=<MeanBackward0>)\n",
      "Step: 230 Loss: tensor(1.5285, grad_fn=<MeanBackward0>)\n",
      "Step: 231 Loss: tensor(1.3487, grad_fn=<MeanBackward0>)\n",
      "Step: 232 Loss: tensor(1.6487, grad_fn=<MeanBackward0>)\n",
      "Step: 233 Loss: tensor(1.3465, grad_fn=<MeanBackward0>)\n",
      "Step: 234 Loss: tensor(1.1671, grad_fn=<MeanBackward0>)\n",
      "Step: 235 Loss: tensor(1.9697, grad_fn=<MeanBackward0>)\n",
      "Step: 236 Loss: tensor(1.7276, grad_fn=<MeanBackward0>)\n",
      "Step: 237 Loss: tensor(1.5614, grad_fn=<MeanBackward0>)\n",
      "Step: 238 Loss: tensor(1.5379, grad_fn=<MeanBackward0>)\n",
      "Step: 239 Loss: tensor(1.1699, grad_fn=<MeanBackward0>)\n",
      "Step: 240 Loss: tensor(1.3384, grad_fn=<MeanBackward0>)\n",
      "Step: 241 Loss: tensor(1.2980, grad_fn=<MeanBackward0>)\n",
      "Step: 242 Loss: tensor(1.7620, grad_fn=<MeanBackward0>)\n",
      "Step: 243 Loss: tensor(1.7471, grad_fn=<MeanBackward0>)\n",
      "Step: 244 Loss: tensor(1.9797, grad_fn=<MeanBackward0>)\n",
      "Step: 245 Loss: tensor(1.8962, grad_fn=<MeanBackward0>)\n",
      "Step: 246 Loss: tensor(1.7168, grad_fn=<MeanBackward0>)\n",
      "Step: 247 Loss: tensor(1.0829, grad_fn=<MeanBackward0>)\n",
      "Step: 248 Loss: tensor(1.4311, grad_fn=<MeanBackward0>)\n",
      "Step: 249 Loss: tensor(1.3838, grad_fn=<MeanBackward0>)\n",
      "Step: 250 Loss: tensor(2.1278, grad_fn=<MeanBackward0>)\n",
      "Step: 251 Loss: tensor(1.4176, grad_fn=<MeanBackward0>)\n",
      "Step: 252 Loss: tensor(1.6837, grad_fn=<MeanBackward0>)\n",
      "Step: 253 Loss: tensor(2.2505, grad_fn=<MeanBackward0>)\n",
      "Step: 254 Loss: tensor(1.4882, grad_fn=<MeanBackward0>)\n",
      "Step: 255 Loss: tensor(1.2433, grad_fn=<MeanBackward0>)\n",
      "Step: 256 Loss: tensor(1.3632, grad_fn=<MeanBackward0>)\n",
      "Step: 257 Loss: tensor(1.7195, grad_fn=<MeanBackward0>)\n",
      "Step: 258 Loss: tensor(1.3725, grad_fn=<MeanBackward0>)\n",
      "Step: 259 Loss: tensor(2.1067, grad_fn=<MeanBackward0>)\n",
      "Step: 260 Loss: tensor(1.4755, grad_fn=<MeanBackward0>)\n",
      "Step: 261 Loss: tensor(1.9884, grad_fn=<MeanBackward0>)\n",
      "Step: 262 Loss: tensor(1.5078, grad_fn=<MeanBackward0>)\n",
      "Step: 263 Loss: tensor(1.5339, grad_fn=<MeanBackward0>)\n",
      "Step: 264 Loss: tensor(2.0667, grad_fn=<MeanBackward0>)\n",
      "Step: 265 Loss: tensor(1.3549, grad_fn=<MeanBackward0>)\n",
      "Step: 266 Loss: tensor(2.0206, grad_fn=<MeanBackward0>)\n",
      "Step: 267 Loss: tensor(1.7243, grad_fn=<MeanBackward0>)\n",
      "Step: 268 Loss: tensor(1.4080, grad_fn=<MeanBackward0>)\n",
      "Step: 269 Loss: tensor(1.3473, grad_fn=<MeanBackward0>)\n",
      "Step: 270 Loss: tensor(1.4412, grad_fn=<MeanBackward0>)\n",
      "Step: 271 Loss: tensor(1.3425, grad_fn=<MeanBackward0>)\n",
      "Step: 272 Loss: tensor(1.3952, grad_fn=<MeanBackward0>)\n",
      "Step: 273 Loss: tensor(1.7975, grad_fn=<MeanBackward0>)\n",
      "Step: 274 Loss: tensor(1.6033, grad_fn=<MeanBackward0>)\n",
      "Step: 275 Loss: tensor(1.1239, grad_fn=<MeanBackward0>)\n",
      "Step: 276 Loss: tensor(1.3383, grad_fn=<MeanBackward0>)\n",
      "Step: 277 Loss: tensor(1.5443, grad_fn=<MeanBackward0>)\n",
      "Step: 278 Loss: tensor(1.9491, grad_fn=<MeanBackward0>)\n",
      "Step: 279 Loss: tensor(2.0849, grad_fn=<MeanBackward0>)\n",
      "Step: 280 Loss: tensor(1.5669, grad_fn=<MeanBackward0>)\n",
      "Step: 281 Loss: tensor(1.4240, grad_fn=<MeanBackward0>)\n",
      "Step: 282 Loss: tensor(1.9029, grad_fn=<MeanBackward0>)\n",
      "Step: 283 Loss: tensor(1.3248, grad_fn=<MeanBackward0>)\n",
      "Step: 284 Loss: tensor(1.6682, grad_fn=<MeanBackward0>)\n",
      "Step: 285 Loss: tensor(2.5738, grad_fn=<MeanBackward0>)\n",
      "Step: 286 Loss: tensor(1.8497, grad_fn=<MeanBackward0>)\n",
      "Step: 287 Loss: tensor(1.4464, grad_fn=<MeanBackward0>)\n",
      "Step: 288 Loss: tensor(1.7210, grad_fn=<MeanBackward0>)\n",
      "Step: 289 Loss: tensor(1.6066, grad_fn=<MeanBackward0>)\n",
      "Step: 290 Loss: tensor(1.7691, grad_fn=<MeanBackward0>)\n",
      "Step: 291 Loss: tensor(1.5714, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 292 Loss: tensor(1.6324, grad_fn=<MeanBackward0>)\n",
      "Step: 293 Loss: tensor(1.2217, grad_fn=<MeanBackward0>)\n",
      "Step: 294 Loss: tensor(1.3654, grad_fn=<MeanBackward0>)\n",
      "Step: 295 Loss: tensor(1.7051, grad_fn=<MeanBackward0>)\n",
      "Step: 296 Loss: tensor(1.5392, grad_fn=<MeanBackward0>)\n",
      "Step: 297 Loss: tensor(1.4763, grad_fn=<MeanBackward0>)\n",
      "Step: 298 Loss: tensor(1.4377, grad_fn=<MeanBackward0>)\n",
      "Step: 299 Loss: tensor(1.4865, grad_fn=<MeanBackward0>)\n",
      "Step: 300 Loss: tensor(1.6215, grad_fn=<MeanBackward0>)\n",
      "Step: 301 Loss: tensor(1.2171, grad_fn=<MeanBackward0>)\n",
      "Step: 302 Loss: tensor(2.0231, grad_fn=<MeanBackward0>)\n",
      "Step: 303 Loss: tensor(1.3109, grad_fn=<MeanBackward0>)\n",
      "Step: 304 Loss: tensor(1.2108, grad_fn=<MeanBackward0>)\n",
      "Step: 305 Loss: tensor(1.4960, grad_fn=<MeanBackward0>)\n",
      "Step: 306 Loss: tensor(1.2785, grad_fn=<MeanBackward0>)\n",
      "Step: 307 Loss: tensor(1.4813, grad_fn=<MeanBackward0>)\n",
      "Step: 308 Loss: tensor(1.4153, grad_fn=<MeanBackward0>)\n",
      "Step: 309 Loss: tensor(1.7325, grad_fn=<MeanBackward0>)\n",
      "Step: 310 Loss: tensor(1.5269, grad_fn=<MeanBackward0>)\n",
      "Step: 311 Loss: tensor(1.8097, grad_fn=<MeanBackward0>)\n",
      "Step: 312 Loss: tensor(1.4939, grad_fn=<MeanBackward0>)\n",
      "Step: 313 Loss: tensor(1.5180, grad_fn=<MeanBackward0>)\n",
      "Step: 314 Loss: tensor(2.1995, grad_fn=<MeanBackward0>)\n",
      "Step: 315 Loss: tensor(1.3571, grad_fn=<MeanBackward0>)\n",
      "Step: 316 Loss: tensor(2.5398, grad_fn=<MeanBackward0>)\n",
      "Step: 317 Loss: tensor(1.5845, grad_fn=<MeanBackward0>)\n",
      "Step: 318 Loss: tensor(1.9734, grad_fn=<MeanBackward0>)\n",
      "Step: 319 Loss: tensor(1.2775, grad_fn=<MeanBackward0>)\n",
      "Step: 320 Loss: tensor(1.2223, grad_fn=<MeanBackward0>)\n",
      "Step: 321 Loss: tensor(1.5545, grad_fn=<MeanBackward0>)\n",
      "Mean: 20.12862\n",
      "Runtime: 21:38:44.436853\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File /projectnb/labci/Lucia/rainfall-pde-ml/experiments/2023-09-18/Bezenac_SqErr_full_0 cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net3\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      4\u001b[0m exp3 \u001b[38;5;241m=\u001b[39m Experiment(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBezenac_SqErr_full_0\u001b[39m\u001b[38;5;124m\"\u001b[39m,                          \u001b[38;5;66;03m# de Bezenac model, trained on err^2 Loss\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                   trainset\u001b[38;5;241m=\u001b[39mtraining_loader, valset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, testset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# data loaders\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                   model\u001b[38;5;241m=\u001b[39mnet3,                                           \u001b[38;5;66;03m# model with 4 days of history\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m                   examples\u001b[38;5;241m=\u001b[39mexample_loader,\n\u001b[1;32m     12\u001b[0m                   outdir\u001b[38;5;241m=\u001b[39mpath)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mexp3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 144\u001b[0m, in \u001b[0;36mExperiment.run\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses\u001b[38;5;241m.\u001b[39mappend(epoch_mean)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# -------------------- Validation ------------------------\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[67], line 111\u001b[0m, in \u001b[0;36mExperiment.save_model_state\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch):\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved checkpoint at epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n",
      "File \u001b[0;32m/projectnb/labci/luciav/.conda/envs/bez-torch/lib/python3.9/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/labci/luciav/.conda/envs/bez-torch/lib/python3.9/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/labci/luciav/.conda/envs/bez-torch/lib/python3.9/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File /projectnb/labci/Lucia/rainfall-pde-ml/experiments/2023-09-18/Bezenac_SqErr_full_0 cannot be opened."
     ]
    }
   ],
   "source": [
    "net3 = CDNN(hist=4)\n",
    "optim = torch.optim.Adam(net3.parameters(), lr=1e-3)\n",
    "\n",
    "exp3 = Experiment(name=\"Bezenac_SqErr_full_0\",                          # de Bezenac model, trained on err^2 Loss\n",
    "                  trainset=training_loader, valset=None, testset=None,  # data loaders\n",
    "                  model=net3,                                           # model with 4 days of history\n",
    "                  loss_fn=squared_error_loss,                           # loss function for training\n",
    "                  regloss=False,                                        # whether to regularize the training loss \n",
    "                  test_loss=nn.MSELoss(reduction=\"mean\"),               # loss function for testing\n",
    "                  optimizer=optim,\n",
    "                  examples=example_loader,\n",
    "                  outdir=path)\n",
    "\n",
    "exp3.run(epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
